{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykhier/Cloud_Course/blob/main/Lab7_RAG1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìö Complete Cell Structure\n",
        "üîß Setup Cells (1-3)\n",
        "\n",
        "Cell 1: Package Installation with progress tracking\n",
        "Cell 2: Import Libraries with fallback detection\n",
        "Cell 3: Vector Store Classes (Simple fallback)\n",
        "\n",
        "üß† Core System Cells (4-6)\n",
        "\n",
        "Cell 4: RAG System Core Class\n",
        "Cell 5: Data Loading Methods\n",
        "Cell 6: Search and Query Methods\n",
        "\n",
        "üìä Data & Interface Cells (7-9)\n",
        "\n",
        "Cell 7: Sample IOLR Data for testing\n",
        "Cell 8: Initialize RAG System\n",
        "Cell 9: Simple Query Interface\n",
        "\n",
        "üîÑ Optional Enhancement Cells (10-11)\n",
        "\n",
        "Cell 10: Load Your Own Papers (optional)\n",
        "Cell 11: Gradio Web Interface (optional)\n",
        "\n",
        "üìà Analytics & Advanced Cells (12-14)\n",
        "\n",
        "Cell 12: Analytics and Evaluation\n",
        "Cell 13: Advanced Query Features\n",
        "Cell 14: System Summary and Testing"
      ],
      "metadata": {
        "id": "hKN2e0GEv4fv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kPovChNs1yY"
      },
      "outputs": [],
      "source": [
        "# CELL 2: Import Libraries and Check Dependencies\n",
        "# ==============================================\n",
        "\"\"\"\n",
        "üìö CELL 2: IMPORT LIBRARIES\n",
        "Run this cell to import all required libraries and check what's available.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any, Optional\n",
        "import re\n",
        "import time\n",
        "\n",
        "# Check what packages are available\n",
        "print(\"üîç Checking available packages...\")\n",
        "\n",
        "# ChromaDB\n",
        "try:\n",
        "    import chromadb\n",
        "    CHROMADB_AVAILABLE = True\n",
        "    print(\"‚úÖ ChromaDB: Available\")\n",
        "except ImportError:\n",
        "    CHROMADB_AVAILABLE = False\n",
        "    print(\"‚ùå ChromaDB: Not available (will use fallback)\")\n",
        "\n",
        "# SentenceTransformers\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    TRANSFORMERS_AVAILABLE = True\n",
        "    print(\"‚úÖ SentenceTransformers: Available\")\n",
        "except ImportError:\n",
        "    TRANSFORMERS_AVAILABLE = False\n",
        "    print(\"‚ùå SentenceTransformers: Not available (will use TF-IDF)\")\n",
        "\n",
        "# OpenAI\n",
        "try:\n",
        "    import openai\n",
        "    OPENAI_AVAILABLE = True\n",
        "    print(\"‚úÖ OpenAI: Available\")\n",
        "except ImportError:\n",
        "    OPENAI_AVAILABLE = False\n",
        "    print(\"‚ùå OpenAI: Not available (will use template responses)\")\n",
        "\n",
        "# Gradio for interface\n",
        "try:\n",
        "    import gradio as gr\n",
        "    GRADIO_AVAILABLE = True\n",
        "    print(\"‚úÖ Gradio: Available\")\n",
        "except ImportError:\n",
        "    GRADIO_AVAILABLE = False\n",
        "    print(\"‚ùå Gradio: Not available (will use simple interface)\")\n",
        "\n",
        "# Fallback imports\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "print(\"\\nüìã System Status:\")\n",
        "print(f\"   Vector DB: {'ChromaDB' if CHROMADB_AVAILABLE else 'Simple Store'}\")\n",
        "print(f\"   Embeddings: {'Transformer' if TRANSFORMERS_AVAILABLE else 'TF-IDF'}\")\n",
        "print(f\"   Generation: {'OpenAI GPT' if OPENAI_AVAILABLE else 'Template'}\")\n",
        "print(f\"   Interface: {'Gradio' if GRADIO_AVAILABLE else 'Simple'}\")\n",
        "\n",
        "print(\"\\nüéØ Ready for Cell 3!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3: Vector Store Classes\n",
        "# ============================\n",
        "\"\"\"\n",
        "üóÑÔ∏è CELL 3: VECTOR STORE CLASSES\n",
        "This cell defines the vector storage classes with fallback options.\n",
        "\"\"\"\n",
        "\n",
        "class SimpleVectorStore:\n",
        "    \"\"\"Fallback vector store when ChromaDB is not available\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.documents = []\n",
        "        self.embeddings = []\n",
        "        self.metadatas = []\n",
        "        self.ids = []\n",
        "        print(\"üì¶ SimpleVectorStore initialized\")\n",
        "\n",
        "    def add(self, embeddings, documents, metadatas, ids):\n",
        "        \"\"\"Add documents to the store\"\"\"\n",
        "        self.embeddings.extend(embeddings)\n",
        "        self.documents.extend(documents)\n",
        "        self.metadatas.extend(metadatas)\n",
        "        self.ids.extend(ids)\n",
        "        print(f\"‚úÖ Added {len(documents)} documents to simple vector store\")\n",
        "\n",
        "    def query(self, query_embeddings, n_results=5):\n",
        "        \"\"\"Query the vector store\"\"\"\n",
        "        if not self.embeddings:\n",
        "            return {'ids': [[]], 'documents': [[]], 'metadatas': [[]], 'distances': [[]]}\n",
        "\n",
        "        # Calculate similarities\n",
        "        similarities = cosine_similarity(query_embeddings, self.embeddings)[0]\n",
        "\n",
        "        # Get top results\n",
        "        top_indices = np.argsort(similarities)[::-1][:n_results]\n",
        "\n",
        "        results = {\n",
        "            'ids': [[self.ids[i] for i in top_indices]],\n",
        "            'documents': [[self.documents[i] for i in top_indices]],\n",
        "            'metadatas': [[self.metadatas[i] for i in top_indices]],\n",
        "            'distances': [[1 - similarities[i] for i in top_indices]]\n",
        "        }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def count(self):\n",
        "        \"\"\"Get count of documents\"\"\"\n",
        "        return len(self.documents)\n",
        "\n",
        "print(\"‚úÖ Vector store classes defined!\")\n",
        "print(\"üìã Next: Run Cell 4 for RAG system core\")"
      ],
      "metadata": {
        "id": "Ua50Y_fFt2jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4: RAG System Core\n",
        "# =======================\n",
        "\"\"\"\n",
        "üß† CELL 4: RAG SYSTEM CORE\n",
        "This cell defines the main EcologicalRAG class.\n",
        "\"\"\"\n",
        "\n",
        "class EcologicalRAG:\n",
        "    \"\"\"Main RAG system for ecological research papers\"\"\"\n",
        "\n",
        "    def __init__(self, openai_api_key=None):\n",
        "        print(\"üåä Initializing Ecological RAG System...\")\n",
        "\n",
        "        # Setup embedding model\n",
        "        if TRANSFORMERS_AVAILABLE:\n",
        "            try:\n",
        "                self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "                self.use_transformers = True\n",
        "                print(\"‚úÖ Loaded SentenceTransformer embeddings\")\n",
        "            except:\n",
        "                self.use_transformers = False\n",
        "                self.tfidf = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "                print(\"‚ö†Ô∏è Using TF-IDF embeddings (fallback)\")\n",
        "        else:\n",
        "            self.use_transformers = False\n",
        "            self.tfidf = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "            print(\"‚ö†Ô∏è Using TF-IDF embeddings\")\n",
        "\n",
        "        # Setup vector store\n",
        "        if CHROMADB_AVAILABLE:\n",
        "            try:\n",
        "                client = chromadb.Client()\n",
        "                try:\n",
        "                    self.collection = client.get_collection(\"ecological_papers\")\n",
        "                    print(\"‚úÖ Loaded existing ChromaDB collection\")\n",
        "                except:\n",
        "                    self.collection = client.create_collection(\"ecological_papers\")\n",
        "                    print(\"‚úÖ Created new ChromaDB collection\")\n",
        "                self.use_chromadb = True\n",
        "            except:\n",
        "                self.collection = SimpleVectorStore()\n",
        "                self.use_chromadb = False\n",
        "                print(\"‚ö†Ô∏è Using simple vector store (fallback)\")\n",
        "        else:\n",
        "            self.collection = SimpleVectorStore()\n",
        "            self.use_chromadb = False\n",
        "            print(\"‚ö†Ô∏è Using simple vector store\")\n",
        "\n",
        "        # Setup OpenAI\n",
        "        if openai_api_key and OPENAI_AVAILABLE:\n",
        "            openai.api_key = openai_api_key\n",
        "            self.use_openai = True\n",
        "            print(\"‚úÖ OpenAI configured\")\n",
        "        else:\n",
        "            self.use_openai = False\n",
        "            print(\"‚ö†Ô∏è Using template responses\")\n",
        "\n",
        "\n",
        "        self.papers = []\n",
        "        self.fitted = False\n",
        "        print(\"üéâ RAG system ready!\")\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"Clean text for better processing\"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = re.sub(r'[^\\w\\s\\-\\.\\(\\)]', ' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def extract_entities(self, text):\n",
        "        \"\"\"Extract ecological entities from text\"\"\"\n",
        "        entities = {'species': [], 'locations': [], 'methods': []}\n",
        "\n",
        "        # Species (binomial nomenclature)\n",
        "        species = re.findall(r'\\b[A-Z][a-z]+ [a-z]+\\b', text)\n",
        "        entities['species'] = list(set(species))[:3]\n",
        "\n",
        "        # Locations\n",
        "        locations = re.findall(r'\\b(Plant|Home plant|Plant disease|Image detection)\\b', text, re.IGNORECASE)\n",
        "        entities['locations'] = list(set(locations))[:3]\n",
        "\n",
        "        # Methods\n",
        "        methods = re.findall(r'\\b(Plant|Disease|sequencing|survey|analysis|modeling)\\b', text, re.IGNORECASE)\n",
        "        entities['methods'] = list(set(methods))[:3]\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def generate_embeddings(self, texts):\n",
        "        \"\"\"Generate embeddings using available method\"\"\"\n",
        "        if self.use_transformers:\n",
        "            return self.embedding_model.encode(texts, show_progress_bar=True)\n",
        "        else:\n",
        "            if not self.fitted:\n",
        "                self.tfidf.fit(texts)\n",
        "                self.fitted = True\n",
        "            return self.tfidf.transform(texts).toarray()\n",
        "\n",
        "print(\"‚úÖ RAG core class defined!\")\n",
        "print(\"üìã Next: Run Cell 5 for data loading methods\")"
      ],
      "metadata": {
        "id": "pnZzNaBWuAwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5: Data Loading Methods\n",
        "# ============================\n",
        "\"\"\"\n",
        "üìö CELL 5: DATA LOADING METHODS\n",
        "This cell adds data loading capabilities to the RAG system.\n",
        "\"\"\"\n",
        "\n",
        "def add_load_papers_method():\n",
        "    \"\"\"Add load_papers method to EcologicalRAG class\"\"\"\n",
        "\n",
        "    def load_papers(self, papers_data):\n",
        "        \"\"\"Load papers into the RAG system\"\"\"\n",
        "        print(f\"üìö Loading {len(papers_data)} papers...\")\n",
        "\n",
        "        valid_papers = [p for p in papers_data if p.get('abstract', '').strip()]\n",
        "        print(f\"üìñ Found {len(valid_papers)} papers with abstracts\")\n",
        "\n",
        "        if not valid_papers:\n",
        "            print(\"‚ùå No valid papers found!\")\n",
        "            return\n",
        "\n",
        "        documents, metadatas, ids = [], [], []\n",
        "\n",
        "        for i, paper in enumerate(valid_papers):\n",
        "            # Combine title and abstract\n",
        "            text = f\"{paper.get('title', '')} {paper.get('abstract', '')}\"\n",
        "            text = self.preprocess_text(text)\n",
        "\n",
        "            if len(text) < 50:\n",
        "                continue\n",
        "\n",
        "            entities = self.extract_entities(text)\n",
        "\n",
        "            metadata = {\n",
        "                'title': paper.get('title', 'Unknown'),\n",
        "                'authors': paper.get('authors', 'Unknown'),\n",
        "                'journal': paper.get('journal', 'Unknown'),\n",
        "                'year': paper.get('year', 2022),\n",
        "                'doi': paper.get('doi', ''),\n",
        "                'species': ', '.join(entities['species']),\n",
        "                'locations': ', '.join(entities['locations']),\n",
        "                'methods': ', '.join(entities['methods'])\n",
        "            }\n",
        "\n",
        "            documents.append(text)\n",
        "            metadatas.append(metadata)\n",
        "            ids.append(f\"paper_{i}\")\n",
        "\n",
        "        if not documents:\n",
        "            print(\"‚ùå No processable documents found!\")\n",
        "            return\n",
        "\n",
        "        # Generate embeddings\n",
        "        print(\"üîÑ Generating embeddings...\")\n",
        "        embeddings = self.generate_embeddings(documents)\n",
        "\n",
        "        # Add to vector store\n",
        "        print(\"üíæ Adding to vector store...\")\n",
        "        if self.use_chromadb:\n",
        "            self.collection.add(\n",
        "                embeddings=embeddings.tolist(),\n",
        "                documents=documents,\n",
        "                metadatas=metadatas,\n",
        "                ids=ids\n",
        "            )\n",
        "        else:\n",
        "            self.collection.add(\n",
        "                embeddings=embeddings,\n",
        "                documents=documents,\n",
        "                metadatas=metadatas,\n",
        "                ids=ids\n",
        "            )\n",
        "\n",
        "        self.papers = valid_papers\n",
        "        print(f\"‚úÖ Successfully loaded {len(documents)} papers!\")\n",
        "\n",
        "    # Add method to class\n",
        "    EcologicalRAG.load_papers = load_papers\n",
        "\n",
        "# Apply the method\n",
        "add_load_papers_method()\n",
        "\n",
        "print(\"‚úÖ Data loading methods added!\")\n",
        "print(\"üìã Next: Run Cell 6 for search and query methods\")"
      ],
      "metadata": {
        "id": "TInkyIKjuGKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 6: Search and Query Methods\n",
        "# ================================\n",
        "\"\"\"\n",
        "üîç CELL 6: SEARCH AND QUERY METHODS\n",
        "This cell adds search and response generation to the RAG system.\n",
        "\"\"\"\n",
        "\n",
        "def add_search_methods():\n",
        "    \"\"\"Add search and query methods to EcologicalRAG class\"\"\"\n",
        "\n",
        "    def search(self, query, n_results=3):\n",
        "        \"\"\"Search for relevant papers\"\"\"\n",
        "        query_processed = self.preprocess_text(query)\n",
        "        query_embedding = self.generate_embeddings([query_processed])\n",
        "\n",
        "        if self.use_chromadb:\n",
        "            results = self.collection.query(\n",
        "                query_embeddings=query_embedding.tolist(),\n",
        "                n_results=n_results\n",
        "            )\n",
        "        else:\n",
        "            results = self.collection.query(\n",
        "                query_embeddings=query_embedding,\n",
        "                n_results=n_results\n",
        "            )\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _generate_openai_response(self, query, papers, search_results):\n",
        "        \"\"\"Generate response using OpenAI\"\"\"\n",
        "        context = \"\\n\\n\".join([\n",
        "            f\"Paper: {papers[i]['title']}\\n\"\n",
        "            f\"Authors: {papers[i]['authors']}\\n\"\n",
        "            f\"Content: {search_results['documents'][0][i][:400]}...\"\n",
        "            for i in range(min(3, len(papers)))\n",
        "        ])\n",
        "\n",
        "        prompt = f\"\"\"You are an expert ecologist. Answer this question based on the research provided:\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Research Papers:\n",
        "{context}\n",
        "\n",
        "Provide a comprehensive answer citing the research. Focus on Plant disease and Home plants ecosystems.\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are an expert ecologist.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                max_tokens=800,\n",
        "                temperature=0.7\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "        except Exception as e:\n",
        "            return f\"OpenAI error: {e}\\n\\nFalling back to template response:\\n\\n{self._generate_template_response(query, papers, search_results)}\"\n",
        "\n",
        "    def _generate_template_response(self, query, papers, search_results):\n",
        "        \"\"\"Generate template response without OpenAI\"\"\"\n",
        "        response = f\"üîç **Search Results for:** {query}\\n\\n\"\n",
        "        response += f\"üìä **Found {len(papers)} relevant papers:**\\n\\n\"\n",
        "\n",
        "        for i, paper in enumerate(papers[:3]):\n",
        "            response += f\"**{i+1}. {paper['title']}**\\n\"\n",
        "            response += f\"   üë• Authors: {paper['authors']}\\n\"\n",
        "            response += f\"   üìñ Journal: {paper['journal']} ({paper['year']})\\n\"\n",
        "\n",
        "            if paper.get('species'):\n",
        "                response += f\"   üêü Species: {paper['species']}\\n\"\n",
        "            if paper.get('locations'):\n",
        "                response += f\"   üìç Locations: {paper['locations']}\\n\"\n",
        "            if paper.get('methods'):\n",
        "                response += f\"   üî¨ Methods: {paper['methods']}\\n\"\n",
        "\n",
        "            response += f\"   üîó DOI: {paper['doi']}\\n\\n\"\n",
        "\n",
        "        # Add summary\n",
        "        all_species = set()\n",
        "        all_locations = set()\n",
        "        for paper in papers:\n",
        "            if paper.get('species'):\n",
        "                all_species.update([s.strip() for s in paper['species'].split(',') if s.strip()])\n",
        "            if paper.get('locations'):\n",
        "                all_locations.update([l.strip() for l in paper['locations'].split(',') if l.strip()])\n",
        "\n",
        "        response += \"üìã **Summary:**\\n\"\n",
        "        if all_species:\n",
        "            response += f\"   üêü Species mentioned: {', '.join(list(all_species)[:5])}\\n\"\n",
        "        if all_locations:\n",
        "            response += f\"   üìç Study areas: {', '.join(list(all_locations))}\\n\"\n",
        "\n",
        "        return response\n",
        "\n",
        "    def generate_response(self, query, search_results):\n",
        "        \"\"\"Generate response based on search results\"\"\"\n",
        "\n",
        "        if not search_results['documents'][0]:\n",
        "            return \"‚ùå No relevant papers found for your query.\"\n",
        "\n",
        "        papers = search_results['metadatas'][0]\n",
        "\n",
        "        if self.use_openai:\n",
        "            return self._generate_openai_response(query, papers, search_results)\n",
        "        else:\n",
        "            return self._generate_template_response(query, papers, search_results)\n",
        "\n",
        "    def query(self, question, n_results=3):\n",
        "        \"\"\"Main query function\"\"\"\n",
        "        print(f\"üîç Processing: {question}\")\n",
        "\n",
        "        search_results = self.search(question, n_results)\n",
        "        response = self.generate_response(question, search_results)\n",
        "\n",
        "        return {\n",
        "            'question': question,\n",
        "            'response': response,\n",
        "            'papers_found': len(search_results['documents'][0]),\n",
        "            'search_results': search_results\n",
        "        }\n",
        "\n",
        "    # Add methods to class\n",
        "    EcologicalRAG.search = search\n",
        "    EcologicalRAG._generate_openai_response = _generate_openai_response\n",
        "    EcologicalRAG._generate_template_response = _generate_template_response\n",
        "    EcologicalRAG.generate_response = generate_response\n",
        "    EcologicalRAG.query = query\n",
        "\n",
        "# Apply the methods\n",
        "add_search_methods()\n",
        "\n",
        "print(\"‚úÖ Search and query methods added!\")\n",
        "print(\"üìã Next: Run Cell 7 for sample data\")"
      ],
      "metadata": {
        "id": "aUUwubeRuIll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 7: Sample Data\n",
        "# ===================\n",
        "\"\"\"\n",
        "üìä CELL 7: SAMPLE DATA\n",
        "This cell provides sample papers for testing the system, now focused on home plants.\n",
        "\"\"\"\n",
        "\n",
        "def get_sample_home_plant_papers():\n",
        "    \"\"\"Get sample papers on identifying home plants\"\"\"\n",
        "\n",
        "    sample_papers = [\n",
        "        {\n",
        "            'title': 'Plant Disease Detection and Classification: A Systematic Review',\n",
        "            'authors': 'Filipe Neves dos Santos',\n",
        "            'journal': 'Journal of Plant Informatics',\n",
        "            'year': 2023,\n",
        "            'doi': '10.3390/s23104769',\n",
        "            'abstract': 'Agricultural productivity is increasingly threatened by plant diseases, which can spread rapidly and lead to significant crop losses if not identified early. Detecting plant diseases accurately in diverse and uncontrolled environments remains challenging, as most current detection methods rely heavily on lab-captured images that may not generalise well to real-world settings. This paper aims to develop models capable of accurately identifying plant diseases across diverse conditions, overcoming the limitations of existing methods. A combined dataset was utilised, incorporating the PlantDoc dataset with web-sourced images of plants from online platforms. State-of-the-art convolutional neural network (CNN) architectures, including EfficientNet-B0, EfficientNet-B3, ResNet50, and DenseNet201, were employed and fine-tuned for plant leaf disease classification. A key contribution of this work is the application of enhanced data augmentation techniques, such as adding Gaussian noise, to improve model generalisation. The results demonstrated varied performance across the datasets. When trained and tested on the PlantDoc dataset, EfficientNet-B3 achieved an accuracy of 73.31%. In cross-dataset evaluation, where the model was trained on PlantDoc and tested on a web-sourced dataset, EfficientNet-B3 reached 76.77% accuracy. The best performance was achieved with the combination of the PlanDoc and web-sourced datasets resulting in an accuracy of 80.19% indicating very good generalisation in diverse conditions. Class-wise F1-scores consistently exceeded 90% for diseases such as apple rust leaf and grape leaf across all models, demonstrating the effectiveness of this approach for plant disease detection.'\n",
        "        },\n",
        "        {\n",
        "            'title': 'Smartphone-Based Image Recognition for On-Site Identification of Ornamental Plants',\n",
        "            'authors': 'Marios Michailidis',\n",
        "            'journal': 'Horticultural Science Research',\n",
        "            'year': 2024,\n",
        "            'doi': '10.3390/electronics13061010',\n",
        "            'abstract': 'This paper investigates the usage of machine learning (ML) algorithms on agricultural images with the aim of extracting information regarding the health of plants. More specifically, a custom convolutional neural network is trained on Google Colab using photos of healthy and unhealthy plants. The trained models are evaluated using various single-board computers (SBCs) that demonstrate different essential characteristics. Raspberry Pi 3 and Raspberry Pi 4 are the current mainstream SBCs that use their Central Processing Units (CPUs) for processing and are used for many applications for executing ML algorithms based on popular related libraries such as TensorFlow. NVIDIA Graphic Processing Units (GPUs) have a different rationale and base the execution of ML algorithms on a GPU that uses a different architecture than a CPU. GPUs can also implement high parallelization on the Compute Unified Device Architecture (CUDA) cores. Another current approach involves using a Tensor Processing Unit (TPU) processing unit carried by the Google Coral Dev TPU Board, which is an Application-Specific Integrated Circuit (ASIC) specialized for accelerating ML algorithms such as Convolutional Neural Networks (CNNs) via the usage of TensorFlow Lite. This study experiments with all of the above-mentioned devices and executes custom CNN models with the aim of identifying plant diseases. In this respect, several evaluation metrics are used, including knowledge extraction time, CPU utilization, Random Access Memory (RAM) usage, swap memory, temperature, current milli Amperes (mA), voltage (Volts), and power consumption milli Watts (mW).'\n",
        "        },\n",
        "        {\n",
        "            'title': 'Plant Leaf Disease Detection Using Deep Learning: A Multi-Dataset Approach',\n",
        "            'authors': 'Manjunatha Shettigere Krishna',\n",
        "            'journal': 'Botanical Genetics',\n",
        "            'year': 2024,\n",
        "            'doi': '10.3390/j8010004',\n",
        "            'abstract': 'Agricultural productivity is increasingly threatened by plant diseases, which can spread rapidly and lead to significant crop losses if not identified early. Detecting plant diseases accurately in diverse and uncontrolled environments remains challenging, as most current detection methods rely heavily on lab-captured images that may not generalise well to real-world settings. This paper aims to develop models capable of accurately identifying plant diseases across diverse conditions, overcoming the limitations of existing methods. A combined dataset was utilised, incorporating the PlantDoc dataset with web-sourced images of plants from online platforms. State-of-the-art convolutional neural network (CNN) architectures, including EfficientNet-B0, EfficientNet-B3, ResNet50, and DenseNet201, were employed and fine-tuned for plant leaf disease classification. A key contribution of this work is the application of enhanced data augmentation techniques, such as adding Gaussian noise, to improve model generalisation. The results demonstrated varied performance across the datasets. When trained and tested on the PlantDoc dataset, EfficientNet-B3 achieved an accuracy of 73.31%. In cross-dataset evaluation, where the model was trained on PlantDoc and tested on a web-sourced dataset, EfficientNet-B3 reached 76.77% accuracy. The best performance was achieved with the combination of the PlanDoc and web-sourced datasets resulting in an accuracy of 80.19% indicating very good generalisation in diverse conditions. Class-wise F1-scores consistently exceeded 90% for diseases such as apple rust leaf and grape leaf across all models, demonstrating the effectiveness of this approach for plant disease detection.'\n",
        "        },\n",
        "        {\n",
        "            'title': 'Plant Leaf Disease Detection Using Deep Learning A Multi-Dataset Approach',\n",
        "            'authors': 'Pedro Machado',\n",
        "            'journal': 'Agricultural AI Journal',\n",
        "            'year': 2024,\n",
        "            'doi': '10.3390/j8010204',\n",
        "            'abstract': 'Agricultural productivity is increasingly threatened by plant diseases, which can spread rapidly and lead to significant crop losses if not identified early. Detecting plant diseases accurately in diverse and uncontrolled environments remains challenging, as most current detection methods rely heavily on lab-captured images that may not generalise well to real-world settings. This paper aims to develop models capable of accurately identifying plant diseases across diverse conditions, overcoming the limitations of existing methods. A combined dataset was utilised, incorporating the PlantDoc dataset with web-sourced images of plants from online platforms. State-of-the-art convolutional neural network (CNN) architectures, including EfficientNet-B0, EfficientNet-B3, ResNet50, and DenseNet201, were employed and fine-tuned for plant leaf disease classification. A key contribution of this work is the application of enhanced data augmentation techniques, such as adding Gaussian noise, to improve model generalisation. The results demonstrated varied performance across the datasets. When trained and tested on the PlantDoc dataset, EfficientNet-B3 achieved an accuracy of 73.31%. In cross-dataset evaluation, where the model was trained on PlantDoc and tested on a web-sourced dataset, EfficientNet-B3 reached 76.77% accuracy. The best performance was achieved with the combination of the PlanDoc and web-sourced datasets resulting in an accuracy of 80.19% indicating very good generalisation in diverse conditions. Class-wise F1-scores consistently exceeded 90% for diseases such as apple rust leaf and grape leaf across all models, demonstrating the effectiveness of this approach for plant disease detection.'\n",
        "\n",
        "        },\n",
        "         {\n",
        "            'title': 'AgriFusionNet: A Lightweight Deep Learning Model for Multisource Plant Disease Diagnosis',\n",
        "            'authors': 'Saleh Albahli',\n",
        "            'journal': 'Agricultural AI Journal',\n",
        "            'year': 2025,\n",
        "            'doi': '10.3390/agriculture15141523',\n",
        "            'abstract': 'Timely and accurate identification of plant diseases is critical to mitigating crop losses and enhancing yield in precision agriculture. This paper proposes AgriFusionNet, a lightweight and efficient deep learning model designed to diagnose plant diseases using multimodal data sources. The framework integrates RGB and multispectral drone imagery with IoT-based environmental sensor data (e.g., temperature, humidity, soil moisture), recorded over six months across multiple agricultural zones. Built on the EfficientNetV2-B4 backbone, AgriFusionNet incorporates Fused-MBConv blocks and Swish activation to improve gradient flow, capture fine-grained disease patterns, and reduce inference latency. The model was evaluated using a comprehensive dataset composed of real-world and benchmarked samples, showing superior performance with 94.3% classification accuracy, 28.5 ms inference time, and a 30% reduction in model parameters compared to state-of-the-art models such as Vision Transformers and InceptionV4. Extensive comparisons with both traditional machine learning and advanced deep learning methods underscore its robustness, generalization, and suitability for deployment on edge devices. Ablation studies and confusion matrix analyses further confirm its diagnostic precision, even in visually ambiguous cases. The proposed framework offers a scalable, practical solution for real-time crop health monitoring, contributing toward smart and sustainable agricultural ecosystems.'\n",
        "\n",
        "         }\n",
        "\n",
        "    ]\n",
        "\n",
        "    print(f\"üìö Loaded {len(sample_papers)} sample home plant papers:\")\n",
        "    for i, paper in enumerate(sample_papers, 1):\n",
        "        print(f\"   {i}. {paper['title'][:60]}...\")\n",
        "\n",
        "    return sample_papers\n",
        "\n",
        "# Load sample data\n",
        "SAMPLE_PAPERS = get_sample_home_plant_papers()\n",
        "\n",
        "print(\"\\n‚úÖ Sample data ready!\")\n",
        "print(\"üìã Next: Run Cell 8 to initialize RAG system\")"
      ],
      "metadata": {
        "id": "xJbQZhUIuLTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 8: Initialize RAG System\n",
        "# =============================\n",
        "\"\"\"\n",
        "üöÄ CELL 8: INITIALIZE RAG SYSTEM\n",
        "This cell creates the RAG system and loads the sample papers.\n",
        "Set your OpenAI API key here if you have one (optional).\n",
        "\"\"\"\n",
        "\n",
        "# Configuration\n",
        "OPENAI_API_KEY = None  # Replace with your OpenAI API key if you have one\n",
        "# OPENAI_API_KEY = \"sk-your-api-key-here\"  # Uncomment and add your key\n",
        "\n",
        "# Initialize the RAG system\n",
        "print(\"üåä Initializing Ecological RAG System...\")\n",
        "rag_system = EcologicalRAG(openai_api_key=OPENAI_API_KEY)\n",
        "\n",
        "# Load sample papers\n",
        "print(\"\\nüìö Loading sample papers into RAG system...\")\n",
        "rag_system.load_papers(SAMPLE_PAPERS)\n",
        "\n",
        "# Test the system\n",
        "print(\"\\nüß™ Testing system with sample query...\")\n",
        "test_result = rag_system.query(\"What image recognitions algortihms van help in detecting plant diseases?\")\n",
        "print(f\"‚úÖ Test successful! Found {test_result['papers_found']} relevant papers\")\n",
        "\n",
        "print(\"\\nüéâ RAG system is ready!\")\n",
        "print(\"üìã Next: Run Cell 9 for simple interface or Cell 10 to load your own papers\")"
      ],
      "metadata": {
        "id": "FQmFZUkZuPng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 9: Simple Query Interface\n",
        "# =============================\n",
        "\"\"\"\n",
        "üí¨ CELL 9: SIMPLE QUERY INTERFACE\n",
        "This cell provides a simple interface to query the RAG system.\n",
        "Copy and run this cell to start asking questions about ecology!\n",
        "\"\"\"\n",
        "\n",
        "def query_interface():\n",
        "    \"\"\"Simple interface for querying the RAG system\"\"\"\n",
        "\n",
        "    print(\"üåä ECOLOGICAL RAG SYSTEM - QUERY INTERFACE\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Ask questions about marine and freshwater ecology!\")\n",
        "    print(\"Type 'quit' to exit, 'help' for examples\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            # Get user input\n",
        "            query = input(\"\\nüîç Your question: \").strip()\n",
        "\n",
        "            if query.lower() == 'quit':\n",
        "                print(\"üëã Goodbye!\")\n",
        "                break\n",
        "\n",
        "            if query.lower() == 'help':\n",
        "                print(\"\\nüí° Example questions:\")\n",
        "                print(\"Here are some sample queries derived from the documents:\")\n",
        "                print(\" ¬† ‚Ä¢ Compare ResNet50 and VGG16 model accuracy.\")\n",
        "                print(\" ¬† ‚Ä¢ What is the final MVGG16 accuracy?\")\n",
        "                print(\" ¬† ‚Ä¢ List plant disease monitoring sensors.\")\n",
        "                print(\" ¬† ‚Ä¢ What prevents model overfitting generally?\")\n",
        "                print(\" ¬† ‚Ä¢ What is the AI-IoT Pivot?\")\n",
        "                print(\" ¬† ‚Ä¢ How many diseases did Mohanty classify?\")\n",
        "                print(\" ¬† ‚Ä¢ Which crop has 100% detection?\")\n",
        "                print(\" ¬† ‚Ä¢ List key mobile app functions.\")\n",
        "\n",
        "            if not query:\n",
        "                print(\"‚ö†Ô∏è Please enter a question\")\n",
        "                continue\n",
        "\n",
        "            # Process query\n",
        "            print(\"\\nüîÑ Searching through research papers...\")\n",
        "            result = rag_system.query(query, n_results=3)\n",
        "\n",
        "            # Display results\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(f\"üìã RESULTS FOR: {query}\")\n",
        "            print(\"=\"*60)\n",
        "            print(result['response'])\n",
        "            print(\"=\"*60)\n",
        "            print(f\"üìä Found {result['papers_found']} relevant papers\")\n",
        "            print(\"üí° Type 'help' for more example questions\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nüëã Goodbye!\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {e}\")\n",
        "            print(\"üí° Try a different question or check if the system is properly initialized\")\n",
        "            continue\n",
        "\n",
        "def single_query(question):\n",
        "    \"\"\"Ask a single question without the interactive loop\"\"\"\n",
        "    try:\n",
        "        print(f\"üîç Searching for: {question}\")\n",
        "        result = rag_system.query(question, n_results=3)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(f\"üìã ANSWER:\")\n",
        "        print(\"=\"*60)\n",
        "        print(result['response'])\n",
        "        print(\"=\"*60)\n",
        "        print(f\"üìä Based on {result['papers_found']} research papers\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Quick test of the interface\n",
        "def test_interface():\n",
        "    \"\"\"Test the interface with sample questions\"\"\"\n",
        "\n",
        "    test_queries = [\n",
        "        \"What is the maximum accuracy achieved by the ResNet50 model for plant disease detection?\",\n",
        "        \"Which deep learning architectures were evaluated for identifying apple leaf diseases?\",\n",
        "        \"What key advantages does the AI-IoT Smart Agriculture Pivot offer over drones and robotics?\",\n",
        "        \"What are the main findings regarding the use of deep learning for image-based plant disease detection in the Mohanty et al. paper[cite: 21]?\",\n",
        "        \"How does the paper by Mahlein et al. describe the use of thermal sensors (IRT) for disease detection[cite: 563]?\",\n",
        "        \"List the components and specifications of the controller used in the proposed hardware pilot[cite: 1301].\",\n",
        "        \"What was the accuracy of the Custom CNN model for classifying potato diseases, and why was it the best model for that crop[cite: 3100]?\"\n",
        "    ]\n",
        "\n",
        "    print(\"üß™ Testing interface with sample questions...\")\n",
        "\n",
        "    for i, question in enumerate(test_questions, 1):\n",
        "        print(f\"\\n[Test {i}/3] {question}\")\n",
        "        result = single_query(question)\n",
        "        if result:\n",
        "            print(f\"‚úÖ Success!\")\n",
        "        else:\n",
        "            print(f\"‚ùå Failed\")\n",
        "\n",
        "    print(\"\\n‚úÖ Interface test completed!\")\n",
        "\n",
        "# Display available functions\n",
        "print(\"‚úÖ Simple interface ready!\")\n",
        "print(\"\\nüöÄ Available functions:\")\n",
        "print(\"   ‚Ä¢ query_interface() - Start interactive questioning\")\n",
        "print(\"   ‚Ä¢ single_query('your question') - Ask one question\")\n",
        "print(\"   ‚Ä¢ test_interface() - Test with sample questions\")\n",
        "\n",
        "print(\"\\nüí° Example usage:\")\n",
        "print(\"   query_interface()  # Start interactive session\")\n",
        "print(\"   single_query(Which crop has 100% detection?')\")\n",
        "\n",
        "print(\"\\nüìã Next: Run Cell 10 to load your own papers (optional)\")"
      ],
      "metadata": {
        "id": "kXDNYzmeuSse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 10: Load Your Own Papers (OPTIONAL)\n",
        "# ========================================\n",
        "\"\"\"\n",
        "üìÅ CELL 10: LOAD YOUR OWN PAPERS (OPTIONAL)\n",
        "Use this cell to load papers you collected with the scraper.\n",
        "Skip this cell if you want to use the sample data.\n",
        "\"\"\"\n",
        "\n",
        "def load_collected_papers(file_path):\n",
        "    \"\"\"Load papers from your collected JSON file\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            papers = json.load(f)\n",
        "\n",
        "        # Filter papers with abstracts\n",
        "        valid_papers = [p for p in papers if p.get('abstract', '').strip()]\n",
        "\n",
        "        print(f\"üìä Loaded {len(papers)} total papers\")\n",
        "        print(f\"‚úÖ Found {len(valid_papers)} papers with abstracts\")\n",
        "\n",
        "        return valid_papers\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå File {file_path} not found\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading papers: {e}\")\n",
        "        return None\n",
        "\n",
        "def analyze_paper_collection(papers):\n",
        "    \"\"\"Analyze the loaded paper collection\"\"\"\n",
        "\n",
        "    if not papers:\n",
        "        print(\"‚ùå No papers to analyze\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nüìä PAPER COLLECTION ANALYSIS\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Basic stats\n",
        "    total_papers = len(papers)\n",
        "    with_abstracts = len([p for p in papers if p.get('abstract', '').strip()])\n",
        "\n",
        "    print(f\"üìö Total papers: {total_papers}\")\n",
        "    print(f\"üìù With abstracts: {with_abstracts}\")\n",
        "    print(f\"üìà Success rate: {with_abstracts/total_papers*100:.1f}%\")\n",
        "\n",
        "    # Journal analysis\n",
        "    journals = [p.get('journal', 'Unknown') for p in papers if p.get('journal')]\n",
        "    if journals:\n",
        "        journal_counts = pd.Series(journals).value_counts()\n",
        "        print(f\"\\nüìñ Top journals:\")\n",
        "        for journal, count in journal_counts.head().items():\n",
        "            print(f\"   ‚Ä¢ {journal}: {count} papers\")\n",
        "\n",
        "    # Abstract length analysis\n",
        "    abstract_lengths = [len(p.get('abstract', '')) for p in papers if p.get('abstract')]\n",
        "    if abstract_lengths:\n",
        "        print(f\"\\nüìè Abstract lengths:\")\n",
        "        print(f\"   ‚Ä¢ Average: {np.mean(abstract_lengths):.0f} characters\")\n",
        "        print(f\"   ‚Ä¢ Range: {min(abstract_lengths)} - {max(abstract_lengths)}\")\n",
        "\n",
        "    print(\"=\"*50)\n",
        "\n",
        "# UNCOMMENT THE LINES BELOW TO LOAD YOUR OWN PAPERS\n",
        "\"\"\"\n",
        "print(\"üìÅ Loading your collected  papers...\")\n",
        "\n",
        "# Replace with your file path\n",
        "your_papers = load_collected_papers('iolr_2022_abstracts_abstracts_only.json')\n",
        "\n",
        "if your_papers:\n",
        "    print(f\"üîÑ Replacing sample data with {len(your_papers)} collected papers...\")\n",
        "\n",
        "    # Analyze the collection\n",
        "    analyze_paper_collection(your_papers)\n",
        "\n",
        "    # Create new RAG system with your papers\n",
        "    rag_system = EcologicalRAG(openai_api_key=OPENAI_API_KEY)\n",
        "    rag_system.load_papers(your_papers)\n",
        "\n",
        "    print(\"‚úÖ Your papers loaded successfully!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Could not load your papers, continuing with sample data\")\n",
        "\"\"\"\n",
        "\n",
        "print(\"üìã Ready to load your own papers!\")\n",
        "print(\"Uncomment the code above and set your file path\")\n",
        "print(\"üìã Next: Run Cell 11 for Gradio interface (optional) or Cell 12 for analytics\")"
      ],
      "metadata": {
        "id": "RvErBkMPvZZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 11: Gradio Web Interface (OPTIONAL)\n",
        "# ========================================\n",
        "\"\"\"\n",
        "üé® CELL 11: GRADIO WEB INTERFACE (OPTIONAL)\n",
        "This cell creates a web-based interface using Gradio.\n",
        "Only run this if Gradio was installed successfully.\n",
        "\"\"\"\n",
        "\n",
        "if GRADIO_AVAILABLE:\n",
        "\n",
        "    def gradio_query(question, n_results=3):\n",
        "        \"\"\"Query function for Gradio interface\"\"\"\n",
        "        if not question.strip():\n",
        "            return \"Please enter a question about ecological research.\"\n",
        "\n",
        "        try:\n",
        "            result = rag_system.query(question, n_results=int(n_results))\n",
        "            return result['response']\n",
        "        except Exception as e:\n",
        "            return f\"Error: {e}\"\n",
        "\n",
        "    def create_gradio_interface():\n",
        "        \"\"\"Create Gradio web interface\"\"\"\n",
        "\n",
        "        # Example questions for the interface\n",
        "        examples = [\n",
        "            [\"Compare ResNet50 and VGG16 model accuracy?\", 3],\n",
        "            [\"What prevents model overfitting generally?\", 3],\n",
        "            [\"Which crop has 100% detection?\", 3],\n",
        "            [\"List plant disease monitoring sensors?\", 3],\n",
        "            [\"What prevents model overfitting generally?\", 3]\n",
        "        ]\n",
        "\n",
        "        # Create interface\n",
        "        interface = gr.Interface(\n",
        "            fn=gradio_query,\n",
        "            inputs=[\n",
        "                gr.Textbox(\n",
        "                    label=\"üîç Ask your ecological question\",\n",
        "                    placeholder=\"e.g., How do we detect plant disease using image detection?\",\n",
        "                    lines=2\n",
        "                ),\n",
        "                gr.Slider(\n",
        "                    minimum=1,\n",
        "                    maximum=5,\n",
        "                    value=3,\n",
        "                    step=1,\n",
        "                    label=\"üìä Number of papers to search\"\n",
        "                )\n",
        "            ],\n",
        "            outputs=gr.Textbox(\n",
        "                label=\"üìã Research-based Answer\",\n",
        "                lines=15\n",
        "            ),\n",
        "            title=\"üåä Ecological RAG System -  Research Assistant\",\n",
        "            description=\"\"\"\n",
        "            Ask questions about marine and freshwater ecology research!\n",
        "            This system searches through reseaech papers\n",
        "            to provide evidence-based answers about home plants ecosystems.\n",
        "            \"\"\",\n",
        "            examples=examples,\n",
        "            theme=gr.themes.Soft()\n",
        "        )\n",
        "\n",
        "        return interface\n",
        "\n",
        "    print(\"üé® Creating Gradio web interface...\")\n",
        "    interface = create_gradio_interface()\n",
        "    interface.launch(share=True)\n",
        "\n",
        "    print(\"üöÄ To launch web interface, run: interface.launch(share=True)\")\n",
        "    print(\"üì± This will open a new tab in your browser\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Gradio not available. Use Cell 9 for simple interface instead.\")\n",
        "\n",
        "print(\"üìã Next: Run Cell 12 for analytics and evaluation\")"
      ],
      "metadata": {
        "id": "kEkK-bjbvbEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 12: Analytics and Evaluation\n",
        "# =================================\n",
        "\"\"\"\n",
        "üìà CELL 12: ANALYTICS AND EVALUATION\n",
        "This cell provides tools to analyze and evaluate RAG system performance.\n",
        "\"\"\"\n",
        "\n",
        "class QueryAnalytics:\n",
        "    \"\"\"Analytics for RAG system queries\"\"\"\n",
        "\n",
        "    def __init__(self, rag_system):\n",
        "        self.rag_system = rag_system\n",
        "        self.query_history = []\n",
        "\n",
        "    def logged_query(self, question, n_results=3):\n",
        "        \"\"\"Query with logging for analytics\"\"\"\n",
        "\n",
        "        start_time = time.time()\n",
        "        result = self.rag_system.query(question, n_results)\n",
        "        end_time = time.time()\n",
        "\n",
        "        # Log the query\n",
        "        log_entry = {\n",
        "            'timestamp': time.time(),\n",
        "            'question': question,\n",
        "            'response_time': end_time - start_time,\n",
        "            'papers_found': result['papers_found'],\n",
        "            'response_length': len(result['response']),\n",
        "            'result': result\n",
        "        }\n",
        "\n",
        "        self.query_history.append(log_entry)\n",
        "        return result\n",
        "\n",
        "    def get_analytics(self):\n",
        "        \"\"\"Get analytics summary\"\"\"\n",
        "\n",
        "        if not self.query_history:\n",
        "            return \"No queries logged yet\"\n",
        "\n",
        "        df = pd.DataFrame(self.query_history)\n",
        "\n",
        "        analytics = {\n",
        "            'total_queries': len(self.query_history),\n",
        "            'avg_response_time': df['response_time'].mean(),\n",
        "            'avg_papers_found': df['papers_found'].mean(),\n",
        "            'avg_response_length': df['response_length'].mean(),\n",
        "            'most_common_topics': self._extract_topics(),\n",
        "            'recent_queries': df.tail(5)['question'].tolist()\n",
        "        }\n",
        "\n",
        "        return analytics\n",
        "\n",
        "    def _extract_topics(self):\n",
        "        \"\"\"Extract common topics from queries\"\"\"\n",
        "        all_queries = ' '.join([q['question'].lower() for q in self.query_history])\n",
        "\n",
        "        # Common ecological terms\n",
        "        topics = {\n",
        "            'DL_Models': sum(q.count('ResNet50') + q.count('VGG16') + q.count('MVGG16') for q in all_queries),\n",
        "            'Accuracy_Metrics': sum(q.count('accuracy') + q.count('100%') for q in all_queries),\n",
        "            'Hardware_Deployment': sum(q.count('Pivot') + q.count('sensors') for q in all_queries),\n",
        "            'Model_Training_Issues': sum(q.count('overfitting') for q in all_queries),\n",
        "            'Study_Scope': sum(q.count('Mohanty') + q.count('diseases') + q.count('crop') for q in all_queries),\n",
        "            'Application_Interface': sum(q.count('mobile app') + q.count('functions') for q in all_queries)\n",
        "        }\n",
        "\n",
        "        return {k: v for k, v in topics.items() if v > 0}\n",
        "\n",
        "    def print_analytics(self):\n",
        "        \"\"\"Print formatted analytics\"\"\"\n",
        "\n",
        "        analytics = self.get_analytics()\n",
        "\n",
        "        if isinstance(analytics, str):\n",
        "            print(analytics)\n",
        "            return\n",
        "\n",
        "        print(\"\\nüìà RAG SYSTEM ANALYTICS\")\n",
        "        print(\"=\"*40)\n",
        "        print(f\"üîç Total queries: {analytics['total_queries']}\")\n",
        "        print(f\"‚è±Ô∏è Avg response time: {analytics['avg_response_time']:.2f}s\")\n",
        "        print(f\"üìö Avg papers found: {analytics['avg_papers_found']:.1f}\")\n",
        "        print(f\"üìù Avg response length: {analytics['avg_response_length']:.0f} chars\")\n",
        "\n",
        "        if analytics['most_common_topics']:\n",
        "            print(f\"\\nüè∑Ô∏è Common topics:\")\n",
        "            for topic, count in analytics['most_common_topics'].items():\n",
        "                print(f\"   ‚Ä¢ {topic}: {count} mentions\")\n",
        "\n",
        "        if analytics['recent_queries']:\n",
        "            print(f\"\\nüïí Recent queries:\")\n",
        "            for i, query in enumerate(analytics['recent_queries'], 1):\n",
        "                print(f\"   {i}. {query[:60]}...\")\n",
        "\n",
        "# Initialize analytics\n",
        "analytics = QueryAnalytics(rag_system)\n",
        "\n",
        "def test_system_performance():\n",
        "    \"\"\"Test system with various queries\"\"\"\n",
        "\n",
        "    test_queries = [\n",
        "        \"What is the final testing accuracy achieved by the ResNet50 model for 11-class plant disease?\",\n",
        "        \"Which CNN models were identified as the best performers for detecting potato and tomato leaf diseases?\",\n",
        "        \"How is the detected plant disease treated or managed by the actuators in the AI-IoT smart agriculture pivot system?\",\n",
        "        \"Describe the key strategies implemented to solve the overfitting issue in the large CNN models used for multi-crop disease detection?\",\n",
        "    ]\n",
        "\n",
        "    print(\"üß™ Testing system performance with sample queries...\")\n",
        "\n",
        "    for i, query in enumerate(test_queries, 1):\n",
        "        print(f\"[{i}/{len(test_queries)}] Testing: {query[:50]}...\")\n",
        "        result = analytics.logged_query(query)\n",
        "        print(f\"   ‚úÖ Found {result['papers_found']} papers\")\n",
        "\n",
        "    print(\"\\nüìä Performance test completed!\")\n",
        "    analytics.print_analytics()\n",
        "\n",
        "print(\"‚úÖ Analytics system ready!\")\n",
        "print(\"üìä Run: test_system_performance() to test with sample queries\")\n",
        "print(\"üìà Run: analytics.print_analytics() to see current stats\")\n",
        "print(\"üìã Next: Run Cell 13 for advanced features\")"
      ],
      "metadata": {
        "id": "tR4ZUrXIvdIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 13: Advanced Query Features\n",
        "# ================================\n",
        "\"\"\"\n",
        "üéØ CELL 13: ADVANCED QUERY FEATURES\n",
        "This cell adds advanced features like query suggestions and filters.\n",
        "\"\"\"\n",
        "\n",
        "class AdvancedQuerySystem:\n",
        "    \"\"\"Enhanced query system with advanced features\"\"\"\n",
        "\n",
        "    def __init__(self, rag_system):\n",
        "        self.rag_system = rag_system\n",
        "        self.common_terms = self._build_term_index()\n",
        "\n",
        "    def _build_term_index(self):\n",
        "        \"\"\"Build index of common terms from papers\"\"\"\n",
        "\n",
        "        terms = {\n",
        "            'DL_models': ['ResNet50', 'VGG16', 'MVGG16', 'Xception', 'DenseNet121', 'InceptionV3', 'MobileNet'],\n",
        "            'crops': ['potato', 'tomato', 'pepper bell', 'apple', 'corn', 'grape', 'peach', 'rice', 'habanero'],\n",
        "            'diseases': ['Early Blight', 'Bacterial Spot', 'Late Blight', 'Yellow Leaf Curl Virus', 'Healthy'],\n",
        "            'architectures': ['CNN', 'Deep Learning', 'Transfer Learning', 'Hybrid Models'],\n",
        "            'systems': ['AI-IoT Pivot', 'Mobile Application', 'FAISS', 'TensorFlow Serving'],\n",
        "            'metrics': ['accuracy', 'precision', 'recall', 'F1-score', 'loss', 'epoch']\n",
        "        }\n",
        "\n",
        "        return terms\n",
        "\n",
        "    def suggest_queries(self, partial_query=\"\"):\n",
        "        \"\"\"Suggest query completions\"\"\"\n",
        "\n",
        "        suggestions = []\n",
        "\n",
        "        # Template-based suggestions\n",
        "        templates = [\n",
        "            \"What are the impacts of {phenomena} on {ecosystems}?\",\n",
        "            \"How do {species} affect {ecosystems}?\",\n",
        "            \"What {methods} are used to study {species}?\",\n",
        "            \"How does climate change affect {species} in the {locations}?\",\n",
        "            \"What causes {phenomena} in {locations}?\"\n",
        "        ]\n",
        "\n",
        "        # Generate suggestions\n",
        "        for template in templates:\n",
        "            for category, terms in self.common_terms.items():\n",
        "                if '{' + category + '}' in template:\n",
        "                    for term in terms[:2]:  # Limit to 2 terms per category\n",
        "                        suggestion = template.replace('{' + category + '}', term)\n",
        "                        # Fill other placeholders with generic terms\n",
        "                        for cat, term_list in self.common_terms.items():\n",
        "                            suggestion = suggestion.replace('{' + cat + '}', term_list[0])\n",
        "                        suggestions.append(suggestion)\n",
        "\n",
        "        # Filter by partial query if provided\n",
        "        if partial_query:\n",
        "            suggestions = [s for s in suggestions if partial_query.lower() in s.lower()]\n",
        "\n",
        "        return list(set(suggestions))[:10]  # Return unique suggestions, max 10\n",
        "\n",
        "    def explain_query(self, question):\n",
        "        \"\"\"Explain how the query will be processed\"\"\"\n",
        "\n",
        "        print(f\"üîç QUERY ANALYSIS: {question}\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Extract key terms\n",
        "        question_lower = question.lower()\n",
        "        found_terms = {}\n",
        "\n",
        "        for category, terms in self.common_terms.items():\n",
        "            found = [term for term in terms if term in question_lower]\n",
        "            if found:\n",
        "                found_terms[category] = found\n",
        "\n",
        "        if found_terms:\n",
        "            print(\"üè∑Ô∏è Detected terms:\")\n",
        "            for category, terms in found_terms.items():\n",
        "                print(f\"   ‚Ä¢ {category.title()}: {', '.join(terms)}\")\n",
        "\n",
        "        # Suggest related queries\n",
        "        suggestions = self.suggest_queries(question)\n",
        "        if suggestions:\n",
        "            print(f\"\\nüí° Related queries you might try:\")\n",
        "            for i, suggestion in enumerate(suggestions[:3], 1):\n",
        "                print(f\"   {i}. {suggestion}\")\n",
        "\n",
        "        print(\"=\"*50)\n",
        "\n",
        "# Initialize advanced query system\n",
        "advanced_query = AdvancedQuerySystem(rag_system)\n",
        "\n",
        "def interactive_query_builder():\n",
        "    \"\"\"Interactive query builder with suggestions\"\"\"\n",
        "\n",
        "    print(\"üéØ ADVANCED QUERY BUILDER\")\n",
        "    print(\"=\"*40)\n",
        "    print(\"Type 'help' for commands, 'quit' to exit\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            command = input(\"\\nüí¨ Command: \").strip().lower()\n",
        "\n",
        "            if command == 'quit':\n",
        "                break\n",
        "\n",
        "            elif command == 'help':\n",
        "                print(\"\\nüìã Available commands:\")\n",
        "                print(\"   ‚Ä¢ suggest - Get query suggestions\")\n",
        "                print(\"   ‚Ä¢ explain <query> - Explain query processing\")\n",
        "                print(\"   ‚Ä¢ query <question> - Regular query\")\n",
        "                print(\"   ‚Ä¢ quit - Exit\")\n",
        "\n",
        "            elif command == 'suggest':\n",
        "                suggestions = advanced_query.suggest_queries()\n",
        "                print(\"\\nüí° Query suggestions:\")\n",
        "                for i, suggestion in enumerate(suggestions[:5], 1):\n",
        "                    print(f\"   {i}. {suggestion}\")\n",
        "\n",
        "            elif command.startswith('explain '):\n",
        "                query = command[8:]\n",
        "                advanced_query.explain_query(query)\n",
        "\n",
        "            elif command.startswith('query '):\n",
        "                question = command[6:]\n",
        "                result = rag_system.query(question)\n",
        "                print(\"\\n\" + \"=\"*50)\n",
        "                print(result['response'])\n",
        "                print(\"=\"*50)\n",
        "\n",
        "            else:\n",
        "                print(\"‚ùì Unknown command. Type 'help' for available commands.\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {e}\")\n",
        "\n",
        "print(\"üéØ Advanced query features loaded!\")\n",
        "print(\"üí° Run: advanced_query.suggest_queries() for suggestions\")\n",
        "print(\"üîç Run: advanced_query.explain_query('your question') for analysis\")\n",
        "print(\"üé® Run: interactive_query_builder() for interactive interface\")\n",
        "print(\"üìã Next: Run Cell 14 for system summary\")"
      ],
      "metadata": {
        "id": "SVGJkE6OvhA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 14: System Summary and Testing\n",
        "# ===================================\n",
        "\"\"\"\n",
        "üìã CELL 14: SYSTEM SUMMARY AND TESTING\n",
        "This cell provides a summary of the complete RAG system and quick tests.\n",
        "\"\"\"\n",
        "\n",
        "def print_system_summary():\n",
        "    \"\"\"Print complete system summary\"\"\"\n",
        "\n",
        "    print(\"üåä ECOLOGICAL RAG SYSTEM - COMPLETE SETUP\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # System status\n",
        "    print(\"üîß SYSTEM STATUS:\")\n",
        "    print(f\"   ‚úÖ Vector Store: {'ChromaDB' if CHROMADB_AVAILABLE else 'Simple Store'}\")\n",
        "    print(f\"   ‚úÖ Embeddings: {'Transformer' if TRANSFORMERS_AVAILABLE else 'TF-IDF'}\")\n",
        "    print(f\"   ‚úÖ Generation: {'OpenAI GPT' if OPENAI_AVAILABLE and rag_system.use_openai else 'Template'}\")\n",
        "    print(f\"   ‚úÖ Interface: {'Gradio' if GRADIO_AVAILABLE else 'Command Line'}\")\n",
        "\n",
        "    # Data status\n",
        "    if hasattr(rag_system, 'collection'):\n",
        "        try:\n",
        "            paper_count = rag_system.collection.count()\n",
        "            print(f\"   ‚úÖ Papers Loaded: {paper_count}\")\n",
        "        except:\n",
        "            print(f\"   ‚úÖ Papers Loaded: {len(rag_system.papers) if hasattr(rag_system, 'papers') else 'Unknown'}\")\n",
        "\n",
        "    # Available functions\n",
        "    print(\"\\nüõ†Ô∏è AVAILABLE FUNCTIONS:\")\n",
        "    print(\"   ‚Ä¢ rag_system.query(question) - Basic query\")\n",
        "    print(\"   ‚Ä¢ analytics.logged_query(question) - Query with analytics\")\n",
        "    print(\"   ‚Ä¢ advanced_query.suggest_queries() - Get suggestions\")\n",
        "    print(\"   ‚Ä¢ query_interface() - Simple text interface\")\n",
        "    print(\"   ‚Ä¢ interactive_query_builder() - Advanced interface\")\n",
        "    if GRADIO_AVAILABLE:\n",
        "        print(\"   ‚Ä¢ interface.launch() - Web interface\")\n",
        "\n",
        "    # Example queries\n",
        "    print(\"\\nüí° EXAMPLE QUERIES:\")\n",
        "    examples = [\n",
        "        \"How do convolutional neural networks improve the accuracy of plant disease identification?\",\n",
        "        \"What challenges arise when detecting crop diseases from images taken under real field conditions?\",\n",
        "        \"How does transfer learning help classify soybean diseases using CNN models like AlexNet and GoogleNet?\",\n",
        "        \"What molecular and serological techniques are used to identify plant pathogens before symptoms appear?\",\n",
        "        \"How has deep learning transformed agricultural plant disease detection compared to traditional image-processing methods?\"\n",
        "    ]\n",
        "\n",
        "    for i, example in enumerate(examples, 1):\n",
        "        print(f\"   {i}. {example}\")\n",
        "\n",
        "    print(\"\\nüéØ QUICK START:\")\n",
        "    print(\"   1. query_interface() - Start asking questions\")\n",
        "    print(\"   2. test_system_performance() - Run performance tests\")\n",
        "    print(\"   3. analytics.print_analytics() - View analytics\")\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"üéâ Your Ecological RAG System is ready!\")\n",
        "    print(\"Happy researching! üåäüî¨üìä\")\n",
        "\n",
        "def quick_test():\n",
        "    \"\"\"Quick test of the system\"\"\"\n",
        "    print(\"\\nüß™ QUICK SYSTEM TEST\")\n",
        "    print(\"-\"*30)\n",
        "\n",
        "    test_questions = [\n",
        "        \"How do variations in environmental conditions (lighting, backgrounds, plant architecture) impact the generalization ability of CNN models trained for plant disease identification?\",\n",
        "        \"What are the main limitations of traditional color- and texture-based image-processing techniques when detecting plant diseases under uncontrolled field conditions?\",\n",
        "        \"How does transfer learning with pretrained CNN architectures like AlexNet and GoogleNet improve the accuracy of soybean disease classification compared to conventional machine-learning methods?\",\n",
        "        \"How do molecular diagnostic tools such as ELISA and FISH complement traditional visual inspection methods in accurately identifying plant pathogens?\",\n",
        "        \"What key challenges must be overcome for deep learning models to fully replace traditional image-processing techniques in real-world agricultural disease detection?\"\n",
        "\n",
        "    ]\n",
        "\n",
        "    for i, question in enumerate(test_questions, 1):\n",
        "        print(f\"\\n[Test {i}] {question}\")\n",
        "        try:\n",
        "            result = rag_system.query(question, n_results=3)\n",
        "            print(f\"‚úÖ Success: Found {result['papers_found']} papers\")\n",
        "            print(f\"üìù Response length: {len(result['response'])} characters\")\n",
        "            # Correctly access paper titles\n",
        "            if result['papers_found'] > 0:\n",
        "                titles = [paper['title'] for paper in result['search_results']['metadatas'][0]]\n",
        "                print(f\"üìù Paper titles: {', '.join(titles)}\")\n",
        "            else:\n",
        "                print(\"üìù No paper titles available.\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {e}\")\n",
        "\n",
        "    print(\"\\n‚úÖ Quick test completed!\")\n",
        "\n",
        "def demo_all_features():\n",
        "    \"\"\"Demonstrate all system features\"\"\"\n",
        "\n",
        "    print(\"üé¨ FULL SYSTEM DEMONSTRATION\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Test basic query\n",
        "    print(\"\\n1Ô∏è‚É£ BASIC QUERY TEST\")\n",
        "    result = rag_system.query(\"How do invasive species affect Mediterranean ecosystems?\")\n",
        "    print(f\"‚úÖ Found {result['papers_found']} papers\")\n",
        "\n",
        "    # Test analytics\n",
        "    print(\"\\n2Ô∏è‚É£ ANALYTICS TEST\")\n",
        "    analytics_result = analytics.logged_query(\"What causes marine heatwaves?\")\n",
        "    print(f\"‚úÖ Analytics logged: {len(analytics.query_history)} total queries\")\n",
        "\n",
        "    # Test suggestions\n",
        "    print(\"\\n3Ô∏è‚É£ QUERY SUGGESTIONS TEST\")\n",
        "    suggestions = advanced_query.suggest_queries()\n",
        "    print(f\"‚úÖ Generated {len(suggestions)} suggestions\")\n",
        "    for i, suggestion in enumerate(suggestions[:3], 1):\n",
        "        print(f\"   {i}. {suggestion}\")\n",
        "\n",
        "    # Test query explanation\n",
        "    print(\"\\n4Ô∏è‚É£ QUERY EXPLANATION TEST\")\n",
        "    advanced_query.explain_query(\"How do jellyfish affect marine ecosystems?\")\n",
        "\n",
        "    print(\"\\nüéâ All features working!\")\n",
        "\n",
        "# Print system summary\n",
        "print_system_summary()\n",
        "\n",
        "quick_test()\n",
        "\n",
        "# Available test functions\n",
        "print(\"\\nüß™ AVAILABLE TESTS:\")\n",
        "print(\"   ‚Ä¢ quick_test() - Quick functionality test\")\n",
        "print(\"   ‚Ä¢ demo_all_features() - Full feature demonstration\")\n",
        "print(\"   ‚Ä¢ test_system_performance() - Comprehensive performance test\")\n",
        "\n",
        "print(\"\\nüöÄ READY TO USE!\")\n",
        "print(\"Run any of the test functions or start with query_interface()\")"
      ],
      "metadata": {
        "id": "OBv8Ky2BvjVU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}